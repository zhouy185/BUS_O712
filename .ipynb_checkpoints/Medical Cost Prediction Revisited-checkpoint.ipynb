{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c830721d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Medical cost prediction revisited\n",
    "\n",
    "Insurance companies need to predict the annual medical cost of a insurance policy holder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92e6c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Target: **annual medical cost**\n",
    "* Predictors: \n",
    "    * age, gender, bmi, number of children, smoker/non-smoke, region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63579376-499b-454b-aa1b-e0fc99148efa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let us load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890962f-7db6-41db-bf0b-a5a65d087c6b",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a118ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's perform **one-hot encoding** for **sex**, **smoker**, **region**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbee0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f847ba9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's seperate the feature variables from the target and perform train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387776c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26f580e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32c658",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create a linear regression on all varaibles. We will:\n",
    "* Fit the model on the training data\n",
    "* Predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa0e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e976d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we will calculate the (root) mean squared error and the $R^2$ score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c7bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e72ee389",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KNN for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31c2cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need the `KNeighborsRegressor()` function from `sklearn.neighbors` to create a KNN model for regression. Recall that we need to specify the number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0468066",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will fit the model on the training data and predict on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd643ab-843c-490c-9c28-e00a6267e0ee",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8981eae",
   "metadata": {},
   "source": [
    "Let's again compute the RMSE and $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e03ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "366e9fad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forest for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03663edf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use the `RandomForestRegressor()` to create a random forest model for regression. The function is included in the module `ensemble` of `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafbdca2-efd4-4363-8778-2af60f9b62ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ea783fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that `RandomForestRegressor()` has similar arguments to `RandomForestClassifier()`. In the above, most arguments are set to their default value. See the [documentation of RandomForestRegressor](https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestRegressor.html) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac737074",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now evaluate the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e16604-0481-47d6-89a7-789e645b3b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39345b70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### XGBOOST for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb2db3",
   "metadata": {},
   "source": [
    "XGBoost stands for Extreme Gradient Boosting and is based on the gradient boosting framework.\n",
    "* Builds decision trees iteratively, with each tree correcting the residuals (errors) of previous trees.\n",
    "* Highly optimized for performance with features like parallel computation, tree pruning (max depth), and out-of-core computing for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9876a342",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "XGBOOST is not part of `sklearn`. Use the following common to install the library (only need to be installed once on your computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06ae13fd-e308-4b03-8e32-c4580fb8777e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.2-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.14.1)\n",
      "Downloading xgboost-2.1.2-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1dc1b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can import the `xgboost` library and use it to create a XGBOOST model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e96b0b-0d0b-4da9-a6eb-f23386bf04bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "801d4283",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the above, we have ommited most of the arguments, effectively setting them to the default values. See the [documentation of XGBOOST](https://xgboost.readthedocs.io/en/stable/parameter.html) for the full list of arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317baa8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use the `.plot_importances()` method of the trained model to visualize the importance of the features.\n",
    "\n",
    "Arguments to this method inculdes:\n",
    "* The trained model\n",
    "* The type of importance:\n",
    "    * `gain`: Average improvement in model performance brought by a feature\n",
    "    * `weight`: Number of times a feature is used in splits\n",
    "    * `cover`: Average number of data samples (instances) impacted by splits that involve a particular feature"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
